{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Importing Training Libraries"
      ],
      "metadata": {
        "id": "OMX3Dn-eWeR7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn, optim\n",
        "import torch.nn.functional as F\n",
        "from torchvision.datasets import MNIST # MNIST\n",
        "from tqdm import tqdm\n",
        "from torchvision import transforms # For Image Augmentation\n",
        "from torchvision.utils import save_image\n",
        "from torch.utils.data import DataLoader # Easier data management by creating mini batches\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "T_Yq-_HEV5FI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Configuration"
      ],
      "metadata": {
        "id": "uicMyLDYWgz9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Model Hyperparameters\n",
        "\n",
        "INPUT_DIM = 784\n",
        "H_DIM = 200\n",
        "Z_DIM = 20\n",
        "NUM_EPOCHS = 6\n",
        "BATCH_SIZE = 64\n",
        "LR_RATE = 3e-4 # Karpathy Constant"
      ],
      "metadata": {
        "id": "hROhzGdPWc0_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Input img -> Hidden dim -> mean, std -> Parametrization trick -> -> Output img [One Hidden Layer]\n",
        "\n",
        "class VAE(nn.Module):\n",
        "    def __init__(self, input_dim, h_dim = 200, z_dim = 20):\n",
        "      super().__init__()\n",
        "      # Encoder\n",
        "      self.img_2hid = nn.Linear(input_dim, h_dim) # Linear layer\n",
        "\n",
        "      # Pushes the layers towards Gaussian, ensuring latent space is Gaussian\n",
        "      self.hid_2mu = nn.Linear(h_dim, z_dim)\n",
        "      self.hid_2sigma = nn.Linear(h_dim, z_dim)\n",
        "\n",
        "      # Decoder\n",
        "      self.z_2hid = nn.Linear(z_dim, h_dim)\n",
        "      self.hid_2img = nn.Linear(h_dim, input_dim)\n",
        "\n",
        "      self.relu = nn.ReLU()\n",
        "      self.training = True\n",
        "\n",
        "\n",
        "    def encode(self, x):\n",
        "      h = self.relu(self.img_2hid(x))\n",
        "      mu, sigma = self.hid_2mu(h), self.hid_2sigma(h)\n",
        "      return mu, sigma\n",
        "\n",
        "    def decode(self, z):\n",
        "      h = self.relu(self.z_2hid(z))\n",
        "      return torch.sigmoid(self.hid_2img(h)) # To ensure pixel vals are binary\n",
        "\n",
        "    def reparameterization(self, mu, sigma):\n",
        "      # Sampling epsilon for latent space with distribution from N(1,2)\n",
        "      # epsilon = torch.randn_like(sigma) * torch.sqrt(torch.tensor(2.0)) + 1\n",
        "\n",
        "      # Sampling epsilon for latent space with distribution from Gamma(3,2)\n",
        "      gamma_distribution = torch.distributions.Gamma(3.0, 2.0)\n",
        "      epsilon = gamma_distribution.sample(sigma.shape).to(sigma.device) # Ensures same device (GPU) as sigma\n",
        "      z = mu + sigma*epsilon                          # Reparameterization trick, Element wise product\n",
        "      return z\n",
        "\n",
        "    def forward(self, x):\n",
        "      mu, sigma = self.encode(x)\n",
        "      epsilon = torch.randn_like(sigma)\n",
        "      z_reparametrized = self.reparameterization(mu, torch.exp(0.5 * sigma))\n",
        "      x_reconstructed = self.decode(z_reparametrized)\n",
        "      return x_reconstructed, mu, sigma"
      ],
      "metadata": {
        "id": "1pTe3gqKSCA9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test Case"
      ],
      "metadata": {
        "id": "H08nVXW7j1FB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "  x = torch.randn(4,28*28)\n",
        "  vae = VAE(input_dim = 784)\n",
        "  x_recon, mu, sigma = vae(x)\n",
        "  print(x_recon.shape)\n",
        "  print(mu.shape)\n",
        "  print(sigma.shape)"
      ],
      "metadata": {
        "id": "EtSeWGdkVOYO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "07183c92-6206-4f71-8462-03d1b8b1168d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([4, 784])\n",
            "torch.Size([4, 20])\n",
            "torch.Size([4, 20])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loading The Dataset"
      ],
      "metadata": {
        "id": "u-DQIFV1X0oV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mnist_transform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "])\n",
        "dataset = MNIST(root=\"dataset/\",\n",
        "                         train = True,\n",
        "                         transform=mnist_transform,\n",
        "                         download=True)\n",
        "train_loader = DataLoader(dataset = dataset,\n",
        "                          batch_size= BATCH_SIZE,\n",
        "                          shuffle = True)\n",
        "model = VAE(INPUT_DIM, H_DIM, Z_DIM)\n",
        "optimizer = optim.Adam(model.parameters(), lr = LR_RATE) # Adam Optimizer"
      ],
      "metadata": {
        "id": "IKQZKweyf_5h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Computer loss\n",
        "def loss_fn(x,x_hat, mean, var):\n",
        "  recon_loss = nn.functional.binary_cross_entropy(x_hat, x, reduction='sum') # Binary Cross Entropy Loss (Since only 2 values 0 or 1 here)\n",
        "  kl_div = -0.5*torch.sum(1 + torch.log(sigma.pow(2)) - mu.pow(2) - sigma.pow(2)) # Minimize the Loss hence negative sign\n",
        "  return recon_loss + kl_div"
      ],
      "metadata": {
        "id": "a-qC8y5gq8Z1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training The Dataset"
      ],
      "metadata": {
        "id": "O27fPFVXYaLK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(NUM_EPOCHS):\n",
        "  overall_loss = 0\n",
        "  loop = tqdm(enumerate(train_loader)) # To get the progress bar\n",
        "  for i, (x, _) in loop:\n",
        "    # Forward pass\n",
        "    x = x.view(x.shape[0], INPUT_DIM)\n",
        "    x_recon, mu, sigma = model(x)\n",
        "    x_recon = torch.clamp(x_recon, 0, 1)\n",
        "\n",
        "    # Backprop\n",
        "    loss = loss_fn(x,x_recon, mu, sigma)\n",
        "    overall_loss += loss.item()\n",
        "\n",
        "    optimizer.zero_grad() # No accumulated gradients from before\n",
        "    loss.backward() # Compute grads\n",
        "    optimizer.step()\n",
        "    loop.set_postfix(loss = loss.item())\n",
        "  print(\"\\tEpoch Num:\", epoch + 1, \"\\tAverage Loss: \", overall_loss/ (i*BATCH_SIZE))"
      ],
      "metadata": {
        "id": "iHkr2K_DYID7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cf855322-53b7-4386-ad45-f38c0e040f33"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "938it [00:19, 47.86it/s, loss=5.19e+3]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\tEpoch Num: 1 \tAverage Loss:  196.0456906281006\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "938it [00:18, 50.67it/s, loss=4.81e+3]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\tEpoch Num: 2 \tAverage Loss:  152.7770244472182\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "938it [00:17, 52.22it/s, loss=4.17e+3]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\tEpoch Num: 3 \tAverage Loss:  142.08048244219833\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "938it [00:18, 50.14it/s, loss=4.49e+3]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\tEpoch Num: 4 \tAverage Loss:  135.57431303856848\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "938it [00:18, 50.99it/s, loss=4e+3]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\tEpoch Num: 5 \tAverage Loss:  132.6663132609527\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "938it [00:20, 45.26it/s, loss=4.17e+3]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\tEpoch Num: 6 \tAverage Loss:  129.42804249017445\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Running The Model On The MNIST Dataset"
      ],
      "metadata": {
        "id": "AKjUpSvKDAJ8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def inference(digit, num_examples=1):\n",
        "  images = []\n",
        "  idx = 0\n",
        "  for x, y in dataset:\n",
        "    if y == idx:\n",
        "      images.append(x)\n",
        "      idx += 1\n",
        "    if idx == 10:\n",
        "      break\n",
        "\n",
        "  # Encoding of Digits\n",
        "  encodings_digit = []\n",
        "  for d in range(10):\n",
        "    with torch.no_grad():\n",
        "      mu, sigma = model.encode(images[d].view(1,784))\n",
        "    encodings_digit.append((mu,sigma))\n",
        "\n",
        "  mu, sigma = encodings_digit[digit]\n",
        "  # Decoding the Digits from the Encodings\n",
        "  for example in range(num_examples):\n",
        "    epsilon = torch.randn_like(sigma)\n",
        "    z = mu + sigma*epsilon\n",
        "    out = model.decode(z)\n",
        "    out = out.view(-1, 1, 28, 28)\n",
        "    save_image(out, f\"generated_{digit}_ex{example}.png\")"
      ],
      "metadata": {
        "id": "OgA2yc6W8duL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for idx in range(10):\n",
        "  inference(idx, num_examples=1)"
      ],
      "metadata": {
        "id": "kYStzEMF95Yt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}